{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import dlib\n",
    "subjects = [\"\", \"Nicke\"]\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "import imutils\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "#convert the test image to gray scale as opencv face detector expects gray images\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "#there is also a more accurate but slow: Haar classifier\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    " \n",
    "#let's detect multiscale images(some images may be closer to camera than others)\n",
    "#result is a list of faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5);\n",
    "    \n",
    "#if no faces are detected then return original img\n",
    "    if (len(faces) == 0):\n",
    "        return None, None\n",
    " \n",
    "#under the assumption that there will be only one face,\n",
    "#extract the face area\n",
    "    (x, y, w, h) = faces[0]\n",
    " \n",
    "#return only the face part of the image\n",
    "    return gray[y:y+w, x:x+h], faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and will return two lists of exactly same size, one list \n",
    "#of faces and another list of labels for each face\n",
    "def prepare_training_data(data_folder_path):\n",
    " \n",
    "#------STEP-1--------\n",
    "#get the directories (one directory for each subject) in data folder\n",
    "    dirs = os.listdir(data_folder_path)\n",
    " \n",
    "#list to hold all subject faces\n",
    "    faces = []\n",
    "#list to hold labels for all subjects\n",
    "    labels = []\n",
    " \n",
    "#let's go through each directory and read images within it\n",
    "    for dir_name in dirs:\n",
    " \n",
    "#our subject directories start with letter 's' so\n",
    "#ignore any non-relevant directories if any\n",
    "        if not dir_name.startswith(\"s\"):\n",
    "            continue;\n",
    " \n",
    "#------STEP-2--------\n",
    "#extract label number of subject from dir_name\n",
    "#format of dir name = slabel\n",
    "#, so removing letter 's' from dir_name will give us label\n",
    "    label = int(dir_name.replace(\"s\", \"\"))\n",
    " \n",
    "#build path of directory containing images for current subject subject\n",
    "#sample subject_dir_path = \"training-data/s1\"\n",
    "    subject_dir_path = data_folder_path + \"/\" + dir_name\n",
    " \n",
    "#get the images names that are inside the given subject directory\n",
    "    subject_images_names = os.listdir(subject_dir_path)\n",
    " \n",
    "#------STEP-3--------\n",
    "#go through each image name, read image, \n",
    "#detect face and add face to list of faces\n",
    "    for image_name in subject_images_names:\n",
    " \n",
    "#ignore system files like .DS_Store\n",
    "        #if image_name.startswith(\".\"):\n",
    "        #continue;\n",
    " \n",
    "#build image path\n",
    "#sample image path = training-data/s1/1.pgm\n",
    "        image_path = subject_dir_path + \"/\" + image_name\n",
    " \n",
    "#read image\n",
    "        image = cv2.imread(image_path)\n",
    " \n",
    "#display an image window to show the image \n",
    "        cv2.imshow(\"Training on image...\", image)\n",
    "        cv2.waitKey(100)\n",
    " \n",
    "#detect face\n",
    "        face, rect = detect_face(image)\n",
    " \n",
    "#------STEP-4--------\n",
    "#for the purpose of this tutorial\n",
    "#we will ignore faces that are not detected\n",
    "        if face is not None:\n",
    "#add face to list of faces\n",
    "            faces.append(face)\n",
    "#add label for this face\n",
    "        labels.append(label)\n",
    " \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    " \n",
    "    return faces, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared\n",
      "Total faces:  10\n",
      "Total labels:  10\n"
     ]
    }
   ],
   "source": [
    "#let's first prepare our training data\n",
    "#data will be in two lists of same size\n",
    "#one list will contain all the faces\n",
    "#and the other list will contain respective labels for each face\n",
    "print(\"Preparing data...\")\n",
    "faces, labels = prepare_training_data(\"training-data\")\n",
    "print(\"Data prepared\")\n",
    " \n",
    "#print total faces and labels\n",
    "print(\"Total faces: \", len(faces))\n",
    "print(\"Total labels: \", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our LBPH face recognizer \n",
    "face_recognizer =cv2.face.LBPHFaceRecognizer_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recognizer.train(faces, np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to draw rectangle on image \n",
    "#according to given (x, y) coordinates and \n",
    "#given width and heigh\n",
    "def draw_rectangle(img, rect):\n",
    " (x, y, w, h) = rect\n",
    " cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    " \n",
    "#function to draw text on give image starting from\n",
    "#passed (x, y) coordinates. \n",
    "def draw_text(img, text, x, y):\n",
    " cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    " \n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    " \n",
    "    # compute the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    " \n",
    "    # return the eye aspect ratio\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function recognizes the person in image passed\n",
    "#and draws the name of the subject \n",
    "#allso draws 68 facial landmarks and uses the eye landmarks to determine if eyes\n",
    "#open or shut.\n",
    "def predict(test_img):\n",
    "    global framecounter\n",
    "    global eyesOpen\n",
    "    global eyesClosed\n",
    "#make a copy of the image as we don't want to change original image\n",
    "    img = test_img.copy()\n",
    "#detect face from the image\n",
    "    face, rect = detect_face(img)\n",
    "    \n",
    "    rects = detector(img,0)\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(img, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(img, (x, y), 2, (0,255,0), -1)\n",
    "            \n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "    \n",
    "    leftEye = shape[lStart:lEnd]\n",
    "    rightEye = shape[rStart:rEnd]\n",
    "    leftEAR = eye_aspect_ratio(leftEye)\n",
    "    rightEAR = eye_aspect_ratio(rightEye)\n",
    "    #ear = a value for the \"open-ness of the eyes\"\n",
    "    ear = (leftEAR + rightEAR) / 2.0\n",
    "    #found that an ear value belov 0.3 = closed eyes\n",
    "    if ear < 0.3:\n",
    "        #add 2 counters (one for alert and one for PERCLOS) if eyes are\n",
    "        #considered closed to count the number \n",
    "        #of frames (ear value less then 0.3)\n",
    "        framecounter +=1\n",
    "        eyesClosed +=1\n",
    "        draw_text(img, \"eyes closed\", x-200, y-250)\n",
    "        \n",
    "    elif ear >0.3:\n",
    "        eyesOpen +=1\n",
    "        framecounter =0\n",
    "        draw_text(img, \"eyes open\", x-200, y-250)\n",
    "    print (ear)\n",
    "    print (framecounter)\n",
    "    #a blinck checker (a blink is less than 7 frames long.)\n",
    "    #if not a blink, plays a \"wakeup sound\"\n",
    "    if framecounter >=7:\n",
    "        winsound.PlaySound('airHorn.wav', winsound.SND_FILENAME)\n",
    "        framecounter = 0\n",
    "    #calculates the % of time that eyes are closed, can be\n",
    "    #an indicator of tiredness.\n",
    "    PERCLOS = (eyesClosed/(eyesOpen+eyesClosed)) * 100\n",
    "    #because PERCLOSE is't a value untill the eyes has been closed atleast\n",
    "    #once, this if statement is needed.\n",
    "    if PERCLOS >0:\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, \"PERCLOS \"+str(PERCLOS) ,(500,700), font, 1,(0,0,255),2,cv2.LINE_AA)\n",
    "    \n",
    "            \n",
    "#predict the image using our face recognizer \n",
    "    label = face_recognizer.predict(face)\n",
    "#get name of respective label returned by face recognizer\n",
    "    label_text = subjects[label[0]]\n",
    "\n",
    "    \n",
    "#draw name of predicted person\n",
    "    \n",
    "    draw_text(img, label_text, x-200, y-200)\n",
    " \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing...\n",
      "0.3503412052586733\n",
      "0\n",
      "0.361321330276142\n",
      "0\n",
      "0.35506518622443906\n",
      "0\n",
      "0.33232336300160803\n",
      "0\n",
      "0.35738620125994564\n",
      "0\n",
      "0.34684225903851795\n",
      "0\n",
      "0.35973705595437927\n",
      "0\n",
      "0.3591658056949335\n",
      "0\n",
      "0.36240853662917466\n",
      "0\n",
      "0.3454064289613801\n",
      "0\n",
      "0.34361151675596624\n",
      "0\n",
      "0.344540029027672\n",
      "0\n",
      "0.3449235511502659\n",
      "0\n",
      "0.3533264214621678\n",
      "0\n",
      "0.3545271653667467\n",
      "0\n",
      "0.3467669729066155\n",
      "0\n",
      "0.33896480881993396\n",
      "0\n",
      "0.3421446767802615\n",
      "0\n",
      "0.34233314456372255\n",
      "0\n",
      "0.35144343400371714\n",
      "0\n",
      "0.33907158070974475\n",
      "0\n",
      "0.33060113678438896\n",
      "0\n",
      "0.3337239955278546\n",
      "0\n",
      "0.33052048920781313\n",
      "0\n",
      "0.3413383401138585\n",
      "0\n",
      "0.3379335987223604\n",
      "0\n",
      "0.3331924014828458\n",
      "0\n",
      "0.33386557787498095\n",
      "0\n",
      "0.35935984648520547\n",
      "0\n",
      "0.3565262039756687\n",
      "0\n",
      "0.36160143137886325\n",
      "0\n",
      "0.373530366362295\n",
      "0\n",
      "0.35360757421144906\n",
      "0\n",
      "0.36381575438062097\n",
      "0\n",
      "0.3650287263079006\n",
      "0\n",
      "0.3725092655682406\n",
      "0\n",
      "0.36022999713682524\n",
      "0\n",
      "0.36323090527520574\n",
      "0\n",
      "0.33628622485974763\n",
      "0\n",
      "0.16946630475287328\n",
      "1\n",
      "0.15905473891863606\n",
      "2\n",
      "0.2184719730813835\n",
      "3\n",
      "0.28619420478156454\n",
      "4\n",
      "0.29774064458525273\n",
      "5\n",
      "0.30016909084421184\n",
      "0\n",
      "0.30563929667213285\n",
      "0\n",
      "0.32743034861733616\n",
      "0\n",
      "0.32092766302360537\n",
      "0\n",
      "0.32112659046885494\n",
      "0\n",
      "0.33156719757915976\n",
      "0\n",
      "0.32586918918796265\n",
      "0\n",
      "0.3230077261358039\n",
      "0\n",
      "0.3257734235818506\n",
      "0\n",
      "0.34823448278687363\n",
      "0\n",
      "0.369451177652666\n",
      "0\n",
      "0.35179903470295437\n",
      "0\n",
      "0.37009267132158835\n",
      "0\n",
      "0.34532622475242053\n",
      "0\n",
      "0.36398210472088055\n",
      "0\n",
      "0.37092302009335176\n",
      "0\n",
      "0.3789083894497673\n",
      "0\n",
      "0.3618785365980812\n",
      "0\n",
      "0.3697358346102673\n",
      "0\n",
      "0.37025999798169107\n",
      "0\n",
      "0.36576943183479005\n",
      "0\n",
      "0.35966805324024875\n",
      "0\n",
      "0.3377601803421574\n",
      "0\n",
      "0.3635724028216123\n",
      "0\n",
      "0.3447791266965756\n",
      "0\n",
      "0.36292965107236863\n",
      "0\n",
      "0.36089756905859516\n",
      "0\n",
      "0.3489965310577364\n",
      "0\n",
      "0.3553659171315022\n",
      "0\n",
      "0.3541342007128054\n",
      "0\n",
      "0.35194373205512064\n",
      "0\n",
      "0.35572037158312986\n",
      "0\n",
      "0.36288647241087935\n",
      "0\n",
      "0.3432947518602778\n",
      "0\n",
      "0.2088599551264784\n",
      "1\n",
      "0.18644772879336785\n",
      "2\n",
      "0.25355743145218207\n",
      "3\n",
      "0.2271009965819496\n",
      "4\n",
      "0.17201281655670655\n",
      "5\n",
      "0.18829913304186324\n",
      "6\n",
      "0.2353039200660286\n",
      "7\n",
      "0.22701204307491885\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"runing...\")\n",
    "\n",
    "global framecounter\n",
    "framecounter = 0\n",
    "eyesOpen =0\n",
    "eyesClosed= 0\n",
    "p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "#start dlib's face detector and create the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(p)\n",
    "#read the video file\n",
    "video_capture = cv2.VideoCapture('test.mp4')\n",
    " \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    #run the frame through the prediction function\n",
    "    predicted_img = predict(frame)\n",
    "    #display the name of the detected person\n",
    "    cv2.imshow(subjects[0], predicted_img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
